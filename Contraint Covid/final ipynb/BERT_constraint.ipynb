{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "BERT_constraint.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxfFhy59BiJE",
        "outputId": "9de21e0d-96f8-40e0-ba67-a26b3ef53efb"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNwTQef3Zs_-",
        "outputId": "bccd0151-31de-4787-92be-a35179aea2c2"
      },
      "source": [
        "!pip install pytorch-transformers\n",
        "!pip install transformers\n",
        "!pip install nltk\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/b7/d3d18008a67e0b968d1ab93ad444fc05699403fa662f634b2f2c318a508b/pytorch_transformers-1.2.0-py3-none-any.whl (176kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 17.3MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 37.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (1.8.0+cu101)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 50.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (2.23.0)\n",
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/d6/9cd710f83cafa07dee064157c39eed54a863f600e74a01ee9876cdf97c71/boto3-1.17.28-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 54.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->pytorch-transformers) (3.7.4.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch-transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch-transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch-transformers) (1.0.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers) (2020.12.5)\n",
            "Collecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/43/4b4a1b26eb03a429a4c37ca7fdf369d938bd60018fc194e94b8379b0c77c/s3transfer-0.3.4-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.1MB/s \n",
            "\u001b[?25hCollecting botocore<1.21.0,>=1.20.28\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ba/db/dd5c5c7350b32e2f53b7f894aa8b3edf22f24c63ffc69b0a6fd0307f6ef5/botocore-1.20.28-py2.py3-none-any.whl (7.3MB)\n",
            "\u001b[K     |████████████████████████████████| 7.3MB 57.3MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.28->boto3->pytorch-transformers) (2.8.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=2b9c28d33973c2379cd08d35dce0fe9ac528b4996d4242a969e21dab481e0917\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "\u001b[31mERROR: botocore 1.20.28 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: sentencepiece, sacremoses, jmespath, botocore, s3transfer, boto3, pytorch-transformers\n",
            "Successfully installed boto3-1.17.28 botocore-1.20.28 jmespath-0.10.0 pytorch-transformers-1.2.0 s3transfer-0.3.4 sacremoses-0.0.43 sentencepiece-0.1.95\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/54/5ca07ec9569d2f232f3166de5457b63943882f7950ddfcc887732fc7fb23/transformers-4.3.3-py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 18.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 51.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "Successfully installed tokenizers-0.10.1 transformers-4.3.3\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_wQ6zaBBBxG",
        "outputId": "8f748df9-e0bb-4373-e3b1-99c880f568fc"
      },
      "source": [
        "!pip install tweet-preprocessor"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tweet-preprocessor\n",
            "  Downloading https://files.pythonhosted.org/packages/17/9d/71bd016a9edcef8860c607e531f30bd09b13103c7951ae73dd2bf174163c/tweet_preprocessor-0.6.0-py3-none-any.whl\n",
            "Installing collected packages: tweet-preprocessor\n",
            "Successfully installed tweet-preprocessor-0.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4UVghcaWJ6Y",
        "outputId": "732aa53d-c81c-4981-b772-69252382a652"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uP3tG3qRZvja"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import preprocessor as p\n",
        "\n",
        "from transformers import XLMModel, BertTokenizer, BertForSequenceClassification, RobertaTokenizerFast, RobertaForSequenceClassification\n",
        "from transformers import AdamW\n",
        "import nltk\n",
        "from nltk.stem import \tWordNetLemmatizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# % matplotlib inline\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QYohf8eQZvmI",
        "outputId": "f4536ffc-2797-4108-f003-73eb67e72cc6"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Tesla T4'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktFMU77mAuLo"
      },
      "source": [
        "MAX_LEN = 128"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-9BIVc_aJb3"
      },
      "source": [
        "#train=pd.read_csv('/content/gdrive/MyDrive/4th_sem/dataset/constraint_covid/Constraint_English_Train - Sheet1.csv')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ve8nQMK-u_VL"
      },
      "source": [
        "df=pd.read_csv('/content/gdrive/MyDrive/4th_sem/dataset/constraint_covid/Constraint_English_Train - Sheet1.csv')\n",
        "val_df=pd.read_csv('/content/gdrive/MyDrive/4th_sem/dataset/constraint_covid/Constraint_English_Val - Sheet1.csv')\n",
        "test_df=pd.read_csv('/content/gdrive/MyDrive/4th_sem/dataset/constraint_covid/english_test_with_labels - Sheet1.csv')"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlueRHCYBb_G",
        "outputId": "e84d2053-e615-402c-c833-0f6d467cc34c"
      },
      "source": [
        "df = df.reset_index(drop=True)\n",
        "val_df = val_df.reset_index(drop=True)\n",
        "test_df = test_df.reset_index(drop=True)\n",
        "\n",
        "\n",
        "print(\"Size of training set: {}\".format(len(df)))\n",
        "print(\"Size of validation set: {}\".format(len(val_df)))\n",
        "print(\"Size of testing set: {}\".format(len(test_df)))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of training set: 6420\n",
            "Size of validation set: 2140\n",
            "Size of testing set: 2140\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buAhSvXcvX2y"
      },
      "source": [
        "df = df.rename(columns={ 'tweet' : 'text'})\n",
        "df.loc[df['label'] == 'fake', 'label'] = '0'\n",
        "df.loc[df['label'] == 'real', 'label'] = '1'\n",
        "\n",
        "val_df = val_df.rename(columns={ 'tweet' : 'text'})\n",
        "val_df.loc[val_df['label'] == 'fake', 'label'] = '0'\n",
        "val_df.loc[val_df['label'] == 'real', 'label'] = '1'\n",
        "\n",
        "test_df = test_df.rename(columns={ 'tweet' : 'text'})\n",
        "test_df.loc[test_df['label'] == 'fake', 'label'] = '0'\n",
        "test_df.loc[test_df['label'] == 'real', 'label'] = '1'"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "Av3n3BUfvx1A",
        "outputId": "040d3e4a-e8a3-47e6-a8c4-8e34cac727e2"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>The CDC currently reports 99031 deaths. In gen...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>States reported 1121 deaths a small rise from ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Politically Correct Woman (Almost) Uses Pandem...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>#IndiaFightsCorona: We have 1524 #COVID testin...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Populous states can generate large case counts...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id                                               text label\n",
              "0   1  The CDC currently reports 99031 deaths. In gen...     1\n",
              "1   2  States reported 1121 deaths a small rise from ...     1\n",
              "2   3  Politically Correct Woman (Almost) Uses Pandem...     0\n",
              "3   4  #IndiaFightsCorona: We have 1524 #COVID testin...     1\n",
              "4   5  Populous states can generate large case counts...     1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gtf--uEfVE7C"
      },
      "source": [
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "porter_stemmer  = PorterStemmer()\n"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOO9UH0GvVSB"
      },
      "source": [
        "p.set_options(p.OPT.URL, p.OPT.EMOJI)\n",
        "\n",
        "def preprocess(row, lemmatizer, stemmer):\n",
        "    text = row['text']\n",
        "    # text = text.strip('\\xa0')\n",
        "    text = p.clean(text)\n",
        "    tokenization = nltk.word_tokenize(text)     \n",
        "    tokenization = [w for w in tokenization if not w in stop_words]\n",
        "    #   text = ' '.join([porter_stemmer.stem(w) for w in tokenization])\n",
        "    #   text = ' '.join([lemmatizer.lemmatize(w) for w in tokenization])\n",
        "    # text = re.sub(r'\\([0-9]+\\)', '', text).strip()    \n",
        "    return text\n"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnpJ27eIvVU2"
      },
      "source": [
        "df['text'] = df.apply(lambda x: preprocess(x, wordnet_lemmatizer, porter_stemmer), 1)\n",
        "val_df['text'] = val_df.apply(lambda x: preprocess(x, wordnet_lemmatizer, porter_stemmer), 1)\n",
        "test_df['text'] = test_df.apply(lambda x: preprocess(x, wordnet_lemmatizer, porter_stemmer), 1)\n"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dHcpDJmbgaK"
      },
      "source": [
        "def map_label(row):\n",
        "    return 0 if row['label']=='1' else 1\n",
        "\n",
        "df['label_encoded'] = df.apply(lambda x: map_label(x), 1)\n",
        "val_df['label_encoded'] = val_df.apply(lambda x: map_label(x), 1)\n",
        "# test_df['label_encoded'] = test_df.apply(lambda x: map_label(x), 1)\n"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkGKyR6ki-QQ"
      },
      "source": [
        "train_sentences = df.text.values\n",
        "val_sentences = val_df.text.values\n",
        "test_sentences = test_df.text.values\n",
        "\n",
        "train_labels = df.label_encoded.values\n",
        "val_labels = val_df.label_encoded.values\n"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpPMXUDuAuLv"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sm0PDmEaAuLv"
      },
      "source": [
        "def Encode_TextWithAttention(sentence,tokenizer,maxlen,padding_type='max_length',attention_mask_flag=True):\n",
        "    encoded_dict = tokenizer.encode_plus(sentence, add_special_tokens=True, max_length=maxlen, truncation=True, padding=padding_type, return_attention_mask=attention_mask_flag)\n",
        "    return encoded_dict['input_ids'],encoded_dict['attention_mask']\n",
        "\n",
        "def Encode_TextWithoutAttention(sentence,tokenizer,maxlen,padding_type='max_length',attention_mask_flag=False):\n",
        "    encoded_dict = tokenizer.encode_plus(sentence, add_special_tokens=True, max_length=maxlen, truncation=True, padding=padding_type, return_attention_mask=attention_mask_flag)\n",
        "    return encoded_dict['input_ids']\n",
        "\n",
        "def get_TokenizedTextWithAttentionMask(sentenceList, tokenizer):\n",
        "    token_ids_list,attention_mask_list = [],[]\n",
        "    for sentence in sentenceList:\n",
        "        token_ids,attention_mask = Encode_TextWithAttention(sentence,tokenizer,MAX_LEN)\n",
        "        token_ids_list.append(token_ids)\n",
        "        attention_mask_list.append(attention_mask)\n",
        "    return token_ids_list,attention_mask_list\n",
        "\n",
        "def get_TokenizedText(sentenceList, tokenizer):\n",
        "    token_ids_list = []\n",
        "    for sentence in sentenceList:\n",
        "        token_ids = Encode_TextWithoutAttention(sentence,tokenizer,MAX_LEN)\n",
        "        token_ids_list.append(token_ids)\n",
        "    return token_ids_list"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xduasmntAuLx"
      },
      "source": [
        "train_token_ids,train_attention_masks = torch.tensor(get_TokenizedTextWithAttentionMask(train_sentences,tokenizer))\n",
        "val_token_ids,val_attention_masks = torch.tensor(get_TokenizedTextWithAttentionMask(val_sentences,tokenizer))\n",
        "test_token_ids,test_attention_masks = torch.tensor(get_TokenizedTextWithAttentionMask(test_sentences,tokenizer))\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "val_labels = torch.tensor(val_labels)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPlfoGroAuLy"
      },
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_data = TensorDataset(train_token_ids, train_attention_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(val_token_ids, val_attention_masks, val_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "\n",
        "test_data = TensorDataset(test_token_ids, test_attention_masks)\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7H0XxMRdAuLz",
        "outputId": "8b669fba-1355-4d50-8e7c-db96570f91e9"
      },
      "source": [
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).cuda()\n",
        "\n"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEKuaqbuAuL1"
      },
      "source": [
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]\n"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfGCHYibAuL2"
      },
      "source": [
        "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43bJXu0qAuL3"
      },
      "source": [
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Fo5dIo0AuL5",
        "outputId": "d50e34f4-db5d-4028-b635-664e0ad97088"
      },
      "source": [
        "train_loss_set = []\n",
        "best_val_accuracy = 0.50\n",
        "directory_path = '/content/gdrive/MyDrive/4th_sem/dataset/constraint_covid'\n",
        "epochs = 10\n",
        "\n",
        "for _ in trange(epochs, desc=\"Epoch\"):\n",
        "    model.train()\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "  \n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "        loss = outputs[0]\n",
        "        logits = outputs[1]\n",
        "        train_loss_set.append(loss.item())    \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        tr_loss += loss.item()\n",
        "        nb_tr_examples += b_input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "\n",
        "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        with torch.no_grad():\n",
        "          output = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "          logits = output[0]\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
        "    Validation_Accuracy = (eval_accuracy/nb_eval_steps)\n",
        "    if(Validation_Accuracy >= best_val_accuracy):\n",
        "        torch.save(model.state_dict(), directory_path+'/models/BERT_base_uncased_best_model.ckpt')\n",
        "        best_val_accuracy = Validation_Accuracy\n",
        "        print('Model Saved')\n",
        "\n",
        "\n"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:   0%|          | 0/10 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.07593311048089642\n",
            "Validation Accuracy: 0.9640858208955224\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  10%|█         | 1/10 [02:40<24:05, 160.63s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Saved\n",
            "Train loss: 0.033991249032615134\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  20%|██        | 2/10 [05:19<21:19, 159.96s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.960820895522388\n",
            "Train loss: 0.02458902368488819\n",
            "Validation Accuracy: 0.9645522388059702\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  30%|███       | 3/10 [07:58<18:38, 159.84s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Saved\n",
            "Train loss: 0.017593529009067948\n",
            "Validation Accuracy: 0.9734141791044776\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  40%|████      | 4/10 [10:39<16:00, 160.02s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Saved\n",
            "Train loss: 0.004413532269962215\n",
            "Validation Accuracy: 0.9747468017057569\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  50%|█████     | 5/10 [13:18<13:19, 159.99s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Saved\n",
            "Train loss: 0.0005539810672733674\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  60%|██████    | 6/10 [15:57<10:37, 159.42s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.9686833688699359\n",
            "Train loss: 0.012065697910738483\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  70%|███████   | 7/10 [18:35<07:57, 159.08s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.96875\n",
            "Train loss: 0.0063469552767141\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  80%|████████  | 8/10 [21:13<05:17, 158.83s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.9701492537313433\n",
            "Train loss: 0.005873258749054356\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  90%|█████████ | 9/10 [23:51<02:38, 158.56s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.9701492537313433\n",
            "Train loss: 0.003362244228625077\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 100%|██████████| 10/10 [26:29<00:00, 158.98s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.972481343283582\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yys3twKwEM9o"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import preprocessor as p\n",
        "\n",
        "from transformers import XLNetTokenizer, XLNetForSequenceClassification,BertTokenizer, BertForSequenceClassification, DebertaTokenizer, DebertaForSequenceClassification, RobertaTokenizerFast, RobertaForSequenceClassification, ElectraTokenizer, ElectraForSequenceClassification\n",
        "from transformers import AdamW\n",
        "\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bmf7OlaVENDs",
        "outputId": "d80b2ab9-ea78-46c1-eb5c-80c2d1e70444"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)\n"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Tesla T4'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgQkvBweENJl"
      },
      "source": [
        "df=pd.read_csv('/content/gdrive/MyDrive/4th_sem/dataset/constraint_covid/Constraint_English_Train - Sheet1.csv')\n",
        "val_df=pd.read_csv('/content/gdrive/MyDrive/4th_sem/dataset/constraint_covid/Constraint_English_Val - Sheet1.csv')\n",
        "test_df=pd.read_csv('/content/gdrive/MyDrive/4th_sem/dataset/constraint_covid/english_test_with_labels - Sheet1.csv')"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78YWgEE4ENZi",
        "outputId": "7f066955-cf58-4252-ab43-15da91d54543"
      },
      "source": [
        "\n",
        "df = df.reset_index(drop=True)\n",
        "val_df = val_df.reset_index(drop=True)\n",
        "test_df = test_df.reset_index(drop=True)\n",
        "test_label_df = test_df\n",
        "\n",
        "print(\"Size of training set: {}\".format(len(df)))\n",
        "print(\"Size of validation set: {}\".format(len(val_df)))\n",
        "print(\"Size of testing set: {}\".format(len(test_df)))"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of training set: 6420\n",
            "Size of validation set: 2140\n",
            "Size of testing set: 2140\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJFW6IvjENO_"
      },
      "source": [
        "df = df.rename(columns={ 'tweet' : 'text'})\n",
        "df.loc[df['label'] == 'fake', 'label'] = '0'\n",
        "df.loc[df['label'] == 'real', 'label'] = '1'\n",
        "\n",
        "val_df = val_df.rename(columns={ 'tweet' : 'text'})\n",
        "val_df.loc[val_df['label'] == 'fake', 'label'] = '0'\n",
        "val_df.loc[val_df['label'] == 'real', 'label'] = '1'\n",
        "\n",
        "test_df = test_df.rename(columns={ 'tweet' : 'text'})\n",
        "test_df.loc[test_df['label'] == 'fake', 'label'] = '0'\n",
        "test_df.loc[test_df['label'] == 'real', 'label'] = '1'\n",
        "\n",
        "test_label_df = test_df.rename(columns={ 'tweet' : 'text'})\n",
        "test_label_df.loc[test_df['label'] == 'fake', 'label'] = '0'\n",
        "test_label_df.loc[test_df['label'] == 'real', 'label'] = '1'"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "r9cclxZAENU0",
        "outputId": "ad65dce6-4cce-42ff-d9a1-139261290bf3"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>The CDC currently reports 99031 deaths. In gen...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>States reported 1121 deaths a small rise from ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Politically Correct Woman (Almost) Uses Pandem...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>#IndiaFightsCorona: We have 1524 #COVID testin...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Populous states can generate large case counts...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id                                               text label\n",
              "0   1  The CDC currently reports 99031 deaths. In gen...     1\n",
              "1   2  States reported 1121 deaths a small rise from ...     1\n",
              "2   3  Politically Correct Woman (Almost) Uses Pandem...     0\n",
              "3   4  #IndiaFightsCorona: We have 1524 #COVID testin...     1\n",
              "4   5  Populous states can generate large case counts...     1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWNbOgI4ENey"
      },
      "source": [
        "def preprocess(row):\n",
        "  text = row['text']\n",
        "  # text = text.strip('\\xa0')\n",
        "  text = p.clean(text)\n",
        "  # text = re.sub(r'\\([0-9]+\\)', '', text).strip()    \n",
        "  return text\n"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBYAm-PnENjp"
      },
      "source": [
        "def map_label(row):\n",
        "  return 0 if row['label']=='1' else 1\n",
        "\n",
        "df['label_encoded'] = df.apply(lambda x: map_label(x), 1)\n",
        "val_df['label_encoded'] = val_df.apply(lambda x: map_label(x), 1)\n",
        "# test_df['label_encoded'] = test_df.apply(lambda x: map_label(x), 1)\n"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pd5HTg_PAuL8"
      },
      "source": [
        "train_sentences = df.text.values\n",
        "train_token_ids = df.id.values\n",
        "val_sentences = val_df.text.values\n",
        "val_token_ids = val_df.id.values\n",
        "test_sentences = test_df.text.values\n",
        "test_token_ids = test_df.id.values\n"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mD0KlAdAuL8"
      },
      "source": [
        "train_sentences = [sentence + \" [SEP] [CLS]\" for sentence in train_sentences]\n",
        "train_labels = df.label_encoded.values\n",
        "val_sentences = [sentence + \" [SEP] [CLS]\" for sentence in val_sentences]\n",
        "val_labels = val_df.label_encoded.values\n",
        "test_sentences = [sentence + \" [SEP] [CLS]\" for sentence in test_sentences]\n"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNWvsmqGE2Q0"
      },
      "source": [
        "MAX_LEN = 128\n",
        "batch_size = 64\n"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SqRju3yE2U-"
      },
      "source": [
        "def get_dataloader(network, train_sentences, val_sentences, train_labels, val_labels, test_sentences, train_token_ids, val_token_ids, test_token_ids):\n",
        "  dic = {\"XLNET\": \"xlnet-base-cased\", \"BERT\": \"bert-base-uncased\", \"ROBERT\": \"roberta-base\", \"XLM-ROBERT\": \"xlm-roberta-base\", \"DeBERTa\": \"microsoft/deberta-base\", \"Electra\": \"google/electra-base-discriminator\"}\n",
        "  if network=='XLNET':\n",
        "    tokenizer = XLNetTokenizer.from_pretrained(dic[network], do_lower_case=True)\n",
        "  elif network=='DeBERTa':\n",
        "    tokenizer = DebertaTokenizer.from_pretrained(dic[network], do_lower_case=True)\n",
        "  elif network=='BERT':\n",
        "    tokenizer = BertTokenizer.from_pretrained(dic[network], do_lower_case=True)\n",
        "  elif network=='Electra':\n",
        "    tokenizer = ElectraTokenizer.from_pretrained(dic[network], do_lower_case=True)\n",
        "  else:\n",
        "    tokenizer = RobertaTokenizerFast.from_pretrained(dic[network], do_lower_case=True)\n",
        "\n",
        "  tokenized_train_texts = [tokenizer.tokenize(sent) for sent in train_sentences]\n",
        "  print (\"Tokenize the first sentence:\")\n",
        "  print (tokenized_train_texts[0])\n",
        "\n",
        "  tokenized_val_texts = [tokenizer.tokenize(sent) for sent in val_sentences]\n",
        "  tokenized_test_texts = [tokenizer.tokenize(sent) for sent in test_sentences]\n",
        "  input_train_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_train_texts]\n",
        "  input_val_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_val_texts]\n",
        "  input_test_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_test_texts]\n",
        "\n",
        "  input_train_ids = pad_sequences(input_train_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "  input_val_ids = pad_sequences(input_val_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "  input_test_ids = pad_sequences(input_test_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "  # Create attention masks\n",
        "  train_attention_masks = []\n",
        "\n",
        "  # Create a mask of 1s for each token followed by 0s for padding\n",
        "  for seq in input_train_ids:\n",
        "    seq_mask = [float(i>0) for i in seq]\n",
        "    train_attention_masks.append(seq_mask)\n",
        "  train_masks = np.array(train_attention_masks)\n",
        "\n",
        "  # Create attention masks\n",
        "  val_attention_masks = []\n",
        "\n",
        "  # Create a mask of 1s for each token followed by 0s for padding\n",
        "  for seq in input_val_ids:\n",
        "    seq_mask = [float(i>0) for i in seq]\n",
        "    val_attention_masks.append(seq_mask)\n",
        "  validation_masks = np.array(val_attention_masks)\n",
        "\n",
        "  # Create attention masks\n",
        "  test_attention_masks = []\n",
        "\n",
        "  # Create a mask of 1s for each token followed by 0s for padding\n",
        "  for seq in input_test_ids:\n",
        "    seq_mask = [float(i>0) for i in seq]\n",
        "    test_attention_masks.append(seq_mask)\n",
        "  test_masks = np.array(test_attention_masks)\n",
        "\n",
        "  train_inputs = torch.tensor(input_train_ids)\n",
        "  validation_inputs = torch.tensor(input_val_ids)\n",
        "  test_inputs = torch.tensor(input_test_ids)\n",
        "  train_labels = torch.tensor(train_labels)\n",
        "  validation_labels = torch.tensor(val_labels)\n",
        "  train_masks = torch.tensor(train_masks)\n",
        "  validation_masks = torch.tensor(validation_masks)\n",
        "  test_masks = torch.tensor(test_masks)\n",
        "\n",
        "  train_data = TensorDataset(torch.tensor(train_token_ids), train_inputs, train_masks, train_labels)\n",
        "  train_sampler = RandomSampler(train_data)\n",
        "  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "  validation_data = TensorDataset(torch.tensor(val_token_ids), validation_inputs, validation_masks, validation_labels)\n",
        "  validation_sampler = SequentialSampler(validation_data)\n",
        "  validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "\n",
        "  test_data = TensorDataset(torch.tensor(test_token_ids), test_inputs, test_masks)\n",
        "  test_sampler = SequentialSampler(test_data)\n",
        "  test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
        "\n",
        "  return train_dataloader, validation_dataloader, test_dataloader\n"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaAIa6F7E2cW",
        "outputId": "bd4cffc1-56d1-48d9-e21b-75ba82fd3cc3"
      },
      "source": [
        "train_dataloader_BERT, val_dataloader_BERT, test_dataloader_BERT = get_dataloader(\"BERT\", train_sentences, val_sentences, train_labels, val_labels, test_sentences, train_token_ids, val_token_ids, test_token_ids)\n"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenize the first sentence:\n",
            "['the', 'cdc', 'currently', 'reports', '99', '##0', '##31', 'deaths', '.', 'in', 'general', 'the', 'disc', '##re', '##pan', '##cies', 'in', 'death', 'counts', 'between', 'different', 'sources', 'are', 'small', 'and', 'ex', '##pl', '##ica', '##ble', '.', 'the', 'death', 'toll', 'stands', 'at', 'roughly', '1000', '##00', 'people', 'today', '.', '[SEP]', '[CLS]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWAUvx4rE2gs",
        "outputId": "c93ac517-df2b-4217-d353-7ecf16fb712b"
      },
      "source": [
        "model2 = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "directory_path = '/content/gdrive/MyDrive/4th_sem/dataset/constraint_covid/models'\n",
        "model2.load_state_dict(torch.load(directory_path+'/BERT_base_uncased_best_model.ckpt'))\n",
        "model2.eval()\n",
        "model2.cuda()"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ue3ZRy67SeWo"
      },
      "source": [
        "def get_model_preds_labels(model, dataloader, mode='train'):\n",
        "  data_vectors = []\n",
        "  labels = []\n",
        "  ids = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "      correct = 0\n",
        "      total = 0\n",
        "      for i, batch in enumerate(dataloader):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        if mode=='test':\n",
        "            token_ids, b_input_ids, b_input_mask = batch\n",
        "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "            for point, tk_id in zip(F.softmax(outputs.logits).tolist(), token_ids.tolist()):\n",
        "              data_vectors.append(point)       \n",
        "              ids.append(tk_id)     \n",
        "        else:\n",
        "          # Unpack the inputs from our dataloader\n",
        "          token_ids, b_input_ids, b_input_mask, b_labels = batch\n",
        "          # Forward pass\n",
        "          outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "          for point, lab, tk_id in zip(F.softmax(outputs.logits).tolist(), b_labels.tolist(), token_ids.tolist()):\n",
        "            data_vectors.append(point)\n",
        "            labels.append(lab)\n",
        "            ids.append(tk_id)\n",
        "\n",
        "  if mode=='test':\n",
        "    return data_vectors, ids\n",
        "  else:\n",
        "    return data_vectors, labels, ids\n"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMKrY4EDTF2_"
      },
      "source": [
        "#Train Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vg5HupkuS2ai",
        "outputId": "69aa8163-7916-48b8-a5ab-2ba48e01415c"
      },
      "source": [
        "m2, _, train_ids2 = get_model_preds_labels(model2, train_dataloader_BERT)"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "i74CCsMZS2hE",
        "outputId": "00ea714f-53da-4541-b2c6-36aea3917144"
      },
      "source": [
        "pred_labels = ['1' if np.argmax(np.array([each]), 1)[0]==0 else '0' for each in m2]\n",
        "train_pred_df = pd.DataFrame({'id': train_ids2, 'predicted_label': pred_labels})\n",
        "train_pred_df = df[[\"id\", \"label\"]].merge(train_pred_df, on='id', how='left')\n",
        "train_pred_df.head()"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "      <th>predicted_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id label predicted_label\n",
              "0   1     1               1\n",
              "1   2     1               1\n",
              "2   3     0               0\n",
              "3   4     1               1\n",
              "4   5     1               1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROVT0l-nS2mg",
        "outputId": "c1709379-205a-4888-f566-49b0dc0efe8c"
      },
      "source": [
        "train_pred_df.shape"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6420, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXQC_t9hS2sJ"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWfKizGhS2zo",
        "outputId": "3f380b99-f7c4-4c64-e6aa-6883aa8b7128"
      },
      "source": [
        "confusion_matrix(train_pred_df['label'].values, train_pred_df['predicted_label'].values)"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3015,   45],\n",
              "       [   2, 3358]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HqfDZKCS25-",
        "outputId": "ac7fd9ab-bc24-4338-d3eb-344993ce8fd3"
      },
      "source": [
        "accuracy_score(train_pred_df['label'].values, train_pred_df['predicted_label'].values)"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9926791277258566"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYMG3uxOS2_5",
        "outputId": "f2b27623-2949-4559-bd5a-d6ca268a9c41"
      },
      "source": [
        "precision_score(train_pred_df['label'].values, train_pred_df['predicted_label'].values, average='micro')"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9926791277258566"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sribuBxuS3FJ",
        "outputId": "0480c8e3-e6cb-48ad-fe63-5f6c48064bd5"
      },
      "source": [
        "recall_score(train_pred_df['label'].values, train_pred_df['predicted_label'].values, average='micro')"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9926791277258566"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TRKLwz6S3Ll",
        "outputId": "b6fcedb7-6a8f-4d6b-ad6b-9d4e89696150"
      },
      "source": [
        "f1_score(train_pred_df['label'].values, train_pred_df['predicted_label'].values, average='micro')"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9926791277258566"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQm46Ap4TgBE"
      },
      "source": [
        "#Validation Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2DIbdfTS3Ry",
        "outputId": "05b04571-077d-45af-97cb-321dbb79daf2"
      },
      "source": [
        "v2, _, v_ids2 = get_model_preds_labels(model2, val_dataloader_BERT)"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "rZcVzcfVTi3f",
        "outputId": "f597641a-cee7-46a8-d5c4-4736f6e8ac66"
      },
      "source": [
        "val_pred_labels = ['1' if np.argmax(np.array([each]), 1)[0]==0 else '0' for each in v2]\n",
        "val_pred_df = pd.DataFrame({'id': v_ids2, 'predicted_label': val_pred_labels})\n",
        "val_pred_df = val_df[[\"id\", \"label\"]].merge(val_pred_df, on='id', how='left')\n",
        "val_pred_df.head()"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "      <th>predicted_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id label predicted_label\n",
              "0   1     0               0\n",
              "1   2     0               0\n",
              "2   3     0               0\n",
              "3   4     0               0\n",
              "4   5     1               1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeakCbwYTi8_",
        "outputId": "2b4c0471-503a-4bb9-d802-989b4f49ae56"
      },
      "source": [
        "confusion_matrix(val_pred_df['label'].values, val_pred_df['predicted_label'].values)"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 961,   59],\n",
              "       [  12, 1108]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvtKij8rTjF4",
        "outputId": "c53dd456-ff7a-4e66-afce-ad7e13bbb2f9"
      },
      "source": [
        "accuracy_score(val_pred_df['label'].values, val_pred_df['predicted_label'].values)"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9668224299065421"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vs2C6vK1TjMh",
        "outputId": "c60e1e27-58de-4be3-ffdd-d2ad22d4f444"
      },
      "source": [
        "precision_score(val_pred_df['label'].values, val_pred_df['predicted_label'].values, average='micro')"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9668224299065421"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drw0mK5wTjTu",
        "outputId": "bfb60e86-f045-4902-a03d-7e25863056df"
      },
      "source": [
        "recall_score(val_pred_df['label'].values, val_pred_df['predicted_label'].values, average='micro')"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9668224299065421"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZaQhJQd4TjZM",
        "outputId": "fc45cdb4-6904-4f21-e443-3e454dfc3133"
      },
      "source": [
        "f1_score(val_pred_df['label'].values, val_pred_df['predicted_label'].values, average='micro')"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9668224299065421"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vk6xE14bT4rO"
      },
      "source": [
        "#Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bFjpbn1Tjhe",
        "outputId": "45644b5f-84d2-4c31-e1cc-230c3cc77634"
      },
      "source": [
        "t2, t_ids2 = get_model_preds_labels(model2, test_dataloader_BERT, 'test')"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "E7yQLBvuTjua",
        "outputId": "b9870580-d398-47bb-f7b4-bed256221823"
      },
      "source": [
        "test_pred_labels = ['1' if np.argmax(np.array([each]), 1)[0]==0 else '0' for each in t2]\n",
        "test_pred_df = pd.DataFrame({'id': t_ids2, 'predicted_label': test_pred_labels})\n",
        "test_pred_df = test_label_df[[\"id\", \"label\"]].merge(test_pred_df, on='id', how='left')\n",
        "test_pred_df.head()"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "      <th>predicted_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id label predicted_label\n",
              "0   1     1               1\n",
              "1   2     0               0\n",
              "2   3     0               0\n",
              "3   4     1               1\n",
              "4   5     1               1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMSW-qsuTj2l",
        "outputId": "b81a1614-c7ba-4771-dc78-cec526644cda"
      },
      "source": [
        "confusion_matrix(test_pred_df['label'].values, test_pred_df['predicted_label'].values)"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 959,   61],\n",
              "       [  24, 1096]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlWazUrZTj-T",
        "outputId": "74ae690a-70fd-438c-b76d-33628c548ad1"
      },
      "source": [
        "accuracy_score(test_pred_df['label'].values, test_pred_df['predicted_label'].values)"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9602803738317757"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OsgE2V1TkGa",
        "outputId": "849fb770-f9e0-4a78-b30f-630c11ba91c0"
      },
      "source": [
        "precision_score(test_pred_df['label'].values, test_pred_df['predicted_label'].values, average='micro')"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9602803738317757"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mwnPDr3TkOo",
        "outputId": "bddd4878-531d-445b-c5ae-5be72829e071"
      },
      "source": [
        "recall_score(test_pred_df['label'].values, test_pred_df['predicted_label'].values, average='micro')"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9602803738317757"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFcAJ7YQUHxG",
        "outputId": "0d735dde-8a57-4705-9cbf-74da8bde174f"
      },
      "source": [
        "f1_score(test_pred_df['label'].values, test_pred_df['predicted_label'].values, average='micro')"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9602803738317757"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 164
        }
      ]
    }
  ]
}