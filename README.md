# Exploring Transformer Models for Covid-19 Fake News Detection
Due to the successful development and innovations of online social networks in these recent years, fake news is one of the major issues emerging to a large extent that may be based on political purpose or day-to-day life events occurring in social media platforms. Normal users get infected easily with this fake news, which creates a bad effect on our society. So, we aim to improve and build honesty of the information circulating in social networks. Our goal is to look into the methodologies for detection of the fake news collected from social networks and estimating the corresponding performance in this project. We have used pre-trained models like Bidirectional Encoder Representations from Transformers (BERT), Robustly-optimized BERT approach (RoBERTa), DeBERTa (Decoding- enhanced BERT with disentangled attention), XLNet, Unsupervised Cross-lingual Representation Learning at Scale (XLM-RoBERTa), and ELECTRA. We computed test, and validation accuracy for each of the models with different datasets and got a reasonable accuracy.
